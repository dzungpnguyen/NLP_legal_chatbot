{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMi/3CjOO0bdGigNf23eABO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Scraping de Questions et Réponses sur https://answers.justia.com\n","\n","Ce script extrait les URL de chaque catégorie, scrape les Q&A (entre utilisateurs et avocats) de 4 premieres pages, puis enrefistrer les données dans un fichier CSV."],"metadata":{"id":"j9__tVOZwCrB"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"DO8bmRW1vx9N","executionInfo":{"status":"ok","timestamp":1704476842080,"user_tz":-60,"elapsed":11,"user":{"displayName":"Phuong Dung Nguyen","userId":"17175679082881100741"}}},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","# Function to extract category URLs from Justia\n","def get_category_links_from_website(url):\n","    # Read HTML content\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","\n","    links = []\n","    try:\n","        # Find the <ul> containing category links\n","        target_ul = soup.find_all('ul', class_='list-columns list-no-styles list-columns-two list-columns-three')[1]\n","        # Extract URLs\n","        anchors = target_ul.find_all('a', href=True)\n","        links = [a['href'] for a in anchors]\n","        return links\n","    except:\n","        return []"]},{"cell_type":"code","source":["# URL of the website\n","url='https://answers.justia.com'\n","\n","# Get category URLs\n","category_links = get_category_links_from_website(url)\n","category_links = [url + link for link in category_links]\n","\n","len(category_links) # 63"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ste6Q8Lyyf9","executionInfo":{"status":"ok","timestamp":1704476843994,"user_tz":-60,"elapsed":578,"user":{"displayName":"Phuong Dung Nguyen","userId":"17175679082881100741"}},"outputId":"b6029f93-f9ad-4dc7-cf7a-a52daff8396c"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["63"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Function to extract question URLs from each category\n","def get_questions_from_website(url):\n","    # Read category URL\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","\n","    links = []\n","    try:\n","        # Find the div containing question links\n","        target_divs = soup.find_all('div', class_='question-wrapper has-padding-content-block-30 has-negative-sides-30 clearfix')\n","        # Extract links\n","        questions = [div.find('strong', class_='heading-3 has-no-top-margin -question text-soft-wrap') for div in target_divs]\n","        links = [question.find('a', href=True)['href'] for question in questions]\n","        return links\n","    except:\n","        return links"],"metadata":{"id":"GHtnmT9I7Mbz","executionInfo":{"status":"ok","timestamp":1704476844974,"user_tz":-60,"elapsed":6,"user":{"displayName":"Phuong Dung Nguyen","userId":"17175679082881100741"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Apply the function to extract Q&A of the first 4 pages\n","question_links = []\n","\n","for cat_link in category_links:\n","    for i in range(1, 5):\n","        page_link = cat_link + f'?page={i}'\n","        questions = get_questions_from_website(page_link)\n","        question_links += [url + question for question in questions]\n","\n","# Remove duplicate links\n","question_links = list(set(question_links))\n","\n","len(question_links) # 3806"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jx35BpUE1GSe","executionInfo":{"status":"ok","timestamp":1704477169770,"user_tz":-60,"elapsed":131505,"user":{"displayName":"Phuong Dung Nguyen","userId":"17175679082881100741"}},"outputId":"f81a8143-33a2-4f7b-ecdf-fdadf3eb6ad2"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3806"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# Function to extract question and lawyer's answer from each question page\n","def get_question_answer_from_website(url):\n","    # Read page content\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","\n","    question = ''\n","    answer = ''\n","    try:\n","        # Find the div containing question and answer\n","        target_div = soup.find('div', class_='primary-content-wrapper')\n","\n","        # Extract question (including question title and question details)\n","        q_header = target_div.find('h1').get_text()\n","        q_paragraphs = target_div.find('div', class_='question-details to-xlarge-font text-soft-wrap').find_all('p')\n","        q_content = ' '.join(paragraph.get_text() for paragraph in q_paragraphs)\n","        raw_question = f\"{q_header}. {q_content}\"\n","\n","        # Extract answer (only get the first answer)\n","        a_paragraphs = target_div.find('div', class_='answer-detailed-text to-xlarge-font text-soft-wrap').find_all('p')\n","        raw_answer = ' '.join(paragraph.get_text() for paragraph in a_paragraphs)\n","\n","        # Remove \\n and \\t from the texts\n","        question = raw_question.replace('\\n', '').replace('\\t', '')\n","        answer = raw_answer.replace('\\n', '').replace('\\t', '')\n","\n","        return question, answer\n","\n","    except:\n","        return question, answer\n"],"metadata":{"id":"Mc1S39NcBZXC","executionInfo":{"status":"ok","timestamp":1704477175346,"user_tz":-60,"elapsed":210,"user":{"displayName":"Phuong Dung Nguyen","userId":"17175679082881100741"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Initlize dataset\n","data = {'question': [], 'answer': []}\n","\n","# Get questions and answers from question links\n","for q_link in question_links:\n","    question, answer = get_question_answer_from_website(q_link)\n","    data['question'].append(question)\n","    data['answer'].append(answer)\n","\n","# Create a dataFrame\n","df = pd.DataFrame(data)\n","\n","# Remove empty values\n","df = df.replace('', pd.NA).dropna()\n","\n","# Save to csv file\n","df.to_csv('data/legal_qa_justia.csv', index=False)"],"metadata":{"id":"-oO8ar5U_zj9","executionInfo":{"status":"ok","timestamp":1704478501599,"user_tz":-60,"elapsed":1310649,"user":{"displayName":"Phuong Dung Nguyen","userId":"17175679082881100741"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Rw9fzJRtS4cp"},"execution_count":null,"outputs":[]}]}